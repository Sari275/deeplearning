{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNt69S0t4ZGi78EtQ6N8tSJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sari275/my-deep-learning-projects/blob/main/Program_3_Transformer_Based_Audio_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "hhoJT7JKRa_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxF8-YwoRIUP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
        "import torch\n",
        "import torchaudio\n",
        "from glob import glob\n",
        "import os\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Parameters"
      ],
      "metadata": {
        "id": "0rAopTAeRvXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_size = 25\n",
        "input_size = 1\n",
        "batch_size = 8\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "SNuzSAnlRxbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining feature Extraction Using Meta wav2vec2"
      ],
      "metadata": {
        "id": "bpAzd01vSA7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = AutoFeatureExtractor.from_pretrained('facebook/wav2vec2-base')"
      ],
      "metadata": {
        "id": "8YgVF1y9TNJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataset class using PyTorch"
      ],
      "metadata": {
        "id": "Fd508VoHTFft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioData(torch.utils.data.Dataset):\n",
        "    def __init__(self, split):\n",
        "        files = glob(f'/WAVE/projects/CSEN-342-Wi24/data/pr3/{split}/*.wav')\n",
        "        label = f'/WAVE/projects/CSEN-342-Wi24/data/pr3/{split}/labels.txt'\n",
        "        if os.path.isfile(label):\n",
        "            with open(label, 'r') as f:\n",
        "                txt = f.read()\n",
        "                labels = [int(x)-1 for x in txt.split('\\n') if x]\n",
        "        else:\n",
        "            labels = []\n",
        "        audios = []\n",
        "        for file in files:\n",
        "            waveform, samplerate = torchaudio.load(file)\n",
        "            waveform = torchaudio.functional.resample(waveform, samplerate, 16000)\n",
        "            features = torch.tensor(feature_extractor(waveform, sampling_rate=16000)['input_values'][0][0])\n",
        "            audios.append(features)\n",
        "\n",
        "        self.audios = audios\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audios)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labels:\n",
        "            label = torch.tensor(self.labels[idx])\n",
        "            return self.audios[idx], label\n",
        "        return self.audios[idx]"
      ],
      "metadata": {
        "id": "s0CgTwU_TI3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Training Set"
      ],
      "metadata": {
        "id": "AavknpNTT3KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = AudioData('train')"
      ],
      "metadata": {
        "id": "N_sbhU-5T7nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Validation Set"
      ],
      "metadata": {
        "id": "1LZTbHLMT8c-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data = AudioData('val')"
      ],
      "metadata": {
        "id": "C-tQ9D4-T9wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examine one sample from training set"
      ],
      "metadata": {
        "id": "hCty1UfiUWfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio, label = next(iter(train_data))\n",
        "print(audio.shape, label.shape)"
      ],
      "metadata": {
        "id": "UEoq4JhWUYhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader takes training and validation audio datasets and creates a batch of samples."
      ],
      "metadata": {
        "id": "vh17N3GNU28F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Q3EZ3MvMU6BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert audio to chunks of 25 milliseconds and create latent features"
      ],
      "metadata": {
        "id": "ngjrJWlyVgZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForAudioClassification.from_pretrained('facebook/wav2vec2-base', num_labels=output_size)"
      ],
      "metadata": {
        "id": "QBgnsdioViQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send each model parameter to GPU one after the other to be processed"
      ],
      "metadata": {
        "id": "Sm1J9hj9Wo00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "LhRKYEp3WtG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Loss Function"
      ],
      "metadata": {
        "id": "9TI2oCLe928R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "_zLHROKQ94zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Optimizer"
      ],
      "metadata": {
        "id": "K-be0QaG955v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)"
      ],
      "metadata": {
        "id": "n1C6TbGT-DfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Best F1 Score"
      ],
      "metadata": {
        "id": "M0YBvtrp-GAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_f1 = 0"
      ],
      "metadata": {
        "id": "yOLdcchm-X2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training classification model for train set"
      ],
      "metadata": {
        "id": "1nZ11LRb-aoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    train_preds = []\n",
        "    train_trues = []\n",
        "    model.train()\n",
        "    for audios, labels in tqdm(train_loader, total=len(train_loader)):\n",
        "        audios = audios.cuda()\n",
        "        labels = labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(audios).logits\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * audios.size(0)\n",
        "        train_preds.append(preds)\n",
        "        train_trues.append(labels)\n",
        "    train_preds = torch.concat(train_preds)\n",
        "    train_trues = torch.concat(train_trues)"
      ],
      "metadata": {
        "id": "IoLRgE19-fXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go into evaluation mode to do the prediction for the validation set"
      ],
      "metadata": {
        "id": "ul7RyifL_PJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_preds = []\n",
        "    valid_trues = []\n",
        "    for audios, labels in valid_loader:\n",
        "        audios = audios.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = model(audios).logits\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        valid_loss += loss.item() * audios.size(0)  # calculate loss for validation\n",
        "        valid_preds.append(preds)\n",
        "        valid_trues.append(labels)\n",
        "    valid_preds = torch.concat(valid_preds)\n",
        "    valid_trues = torch.concat(valid_trues)\n",
        "\n",
        "    train_loss = train_loss/len(train_loader.dataset) #average loss per sample\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "\n",
        "    valid_preds = valid_preds.detach().cpu().numpy()\n",
        "    valid_trues = valid_trues.detach().cpu().numpy()\n",
        "    train_preds = train_preds.detach().cpu().numpy()\n",
        "    train_trues = train_trues.detach().cpu().numpy()\n",
        "\n",
        "    valid_f1 = f1_score(valid_trues, valid_preds, average='micro')  # calculate f1 predicition for entire dataset\n",
        "    train_f1 = f1_score(train_trues, train_preds, average='micro')\n",
        "\n",
        "    print(f'Epoch = {epoch}, train_loss = {train_loss:.3f}, train_f1 = {train_f1:.3f}, valid_loss = {valid_loss:.3f}, valid_f1 = {valid_f1:.3f}')\n",
        "    if valid_f1 > best_f1:\n",
        "        best_f1 = valid_f1 #keep track of the best f1 validation score\n",
        "        torch.save(model.state_dict(), 'best.pth') #save weights\n",
        "torch.save(model.state_dict(), 'last.pth') #last weights"
      ],
      "metadata": {
        "id": "vlt1CbqI_Sjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a Prediction for the Test Set"
      ],
      "metadata": {
        "id": "vN6I-AqlBYEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = AudioData('test') #declare use of audio files in test folder\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False) #create batch. Dataloader then takes the audio test dataset and creates a batch of samples.\n",
        "model = AutoModelForAudioClassification.from_pretrained('facebook/wav2vec2-base', num_labels=output_size)\n",
        "state = torch.load('best.pth') #use best weights to make predictions for the audios in the test set\n",
        "model.load_state_dict(state) #load the state and update the model with the state\n",
        "model = model.cuda()\n",
        "model.eval()\n",
        "test_preds = [] #create a list to store test_preds\n",
        "for audios in test_loader: # go over each batch of audios\n",
        "    audios = audios.cuda() # pass it through GPU\n",
        "    outputs = model(audios).logits # pass through model\n",
        "    _, preds = torch.max(outputs, 1) # get prediction of model\n",
        "    test_preds.append(preds) # convert probability to classes\n",
        "test_preds = torch.concat(test_preds).detach().cpu().numpy() + 1  #after getting all the batches, concat them all together, therefore the test predictions are brought back from GPU to CPU\n",
        "pd.Series(test_preds).to_csv('predictions.txt', index=False, header=None) #generate prediction .txt file"
      ],
      "metadata": {
        "id": "RNCQgKXWBa_i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}